# LeRobot のデータセットを読み込む

## 方法

1. LeRobot のデータセットを読み込む
    LeRobot では、`LeRobotDataset()` メソッドを使用して `dataset = LeRobotDataset("lerobot/aloha_static_coffee")` のような形式で、LeRobot のデータセットを簡単に読み込むことができるようになっている

    ```python
    # LeRobot のデータセットを読み込む
    dataset = LeRobotDataset("lerobot/aloha_static_coffee")
    # dataset = LeRobotDataset("lerobot/aloha_static_coffee", episodes=[0, 10, 11, 23])
    print("dataset:", dataset)

    # 特定のエピソードと時間ステップのデータを print する
    print("dataset[0]:", dataset[0])
    ```

    LeRobot のデータセットは、以下のように、強化学習用データセットのフォーマットである RLDS [Reinforcement Learning Datasets] と同じようなフォーマットの構造をしている

    ```yaml
    {
    # 観測データ（observation）：ロボットの上部から撮影したカメラの画像
    'observation.images.cam_high': tensor([[[0.9216, 0.8980, 0.8902,  ..., 0.6000, 0.9020, 0.8941],
            [0.9059, 0.9176, 0.9098,  ..., 0.5686, 0.8784, 0.8941],
            [0.8392, 0.9020, 0.9294,  ..., 0.5216, 0.8471, 0.9059],
            ...,
            [0.8196, 0.8157, 0.8157,  ..., 0.5020, 0.5020, 0.5020]]]),
    # 観測データ（observation）：ロボットの左手首に取り付けられたカメラの画像
    'observation.images.cam_left_wrist': tensor([[[0.1569, 0.1608, 0.1608,  ..., 0.2863, 0.3255, 0.3490],
            [0.1647, 0.1608, 0.1529,  ..., 0.2863, 0.3255, 0.3490],
            [0.1804, 0.1569, 0.1373,  ..., 0.2863, 0.3216, 0.3451],
            ...,
            [0.2549, 0.2549, 0.2549,  ..., 0.1176, 0.1176, 0.1176]]]),
    # 観測データ（observation）：ロボットの下部から撮影したカメラ画像
    'observation.images.cam_low': tensor([[[0.9176, 0.9176, 0.9176,  ..., 0.4235, 0.4235, 0.4196],
            [0.9176, 0.9176, 0.9176,  ..., 0.4235, 0.4235, 0.4196],
            [0.9176, 0.9176, 0.9176,  ..., 0.4235, 0.4196, 0.4196],
            ...,
            [0.9098, 0.9098, 0.9098,  ..., 0.5451, 0.5451, 0.5451]]]),
    # 観測データ（observation）：ロボットの右手首に取り付けられたカメラの画像
    'observation.images.cam_right_wrist': tensor([[[0.4745, 0.4706, 0.4706,  ..., 0.6627, 0.6627, 0.6627],
            [0.4706, 0.4667, 0.4627,  ..., 0.6588, 0.6588, 0.6588],
            [0.4667, 0.4667, 0.4627,  ..., 0.6588, 0.6588, 0.6588],
            ...,
            [0.0980, 0.0941, 0.0941,  ..., 0.1059, 0.1059, 0.1059]]]),
    # 観測データ（observation）：状態（state）ベクトル。関節角度や位置情報など
    'observation.state': tensor([-0.0031, -0.9664,  1.1888, -0.0015, -0.2915,  0.0015,  0.0130,  0.0031,
            -0.9695,  1.1873, -0.0015, -0.2899,  0.0015,  0.0044]),
    # 観測データ（observation）：ロボットの各関節にかかる力やトルク情報
    'observation.effort': tensor([   0.0000,  139.8800, -731.6800,    0.0000, -228.6500,   -5.3800,
            -527.2400,  -64.5600,  220.5800, -739.7500,    0.0000, -269.0000,
            -2.6900, -217.8900]),
    # 行動（action）ベクトル：ロボットが次に実行するアクション（関節の目標位置や速度など）
    'action': tensor([-0.0123, -0.9557,  1.1428, -0.0031, -0.2930, -0.0123,  0.1778,  0.0061,
            -0.9541,  1.1551, -0.0015, -0.3037,  0.0031,  0.0588]),
    # エピソードの値
    'episode_index': tensor(0),
    # フレーム（時間ステップ）
    'frame_index': tensor(0),
    # 時間情報
    'timestamp': tensor(0.),
    # エピソードが終了したかどうかのフラグ
    'next.done': tensor(False),
    # データセット内の絶対的なインデックス
    'index': tensor(0),
    # タスクの種類を示すインデックス
    'task_index': tensor(0), 
    # 制御指示テキスト
    'task': "Place the coffee capsule inside the capsule container, then place the cup onto the center of the cup tray, then push the 'Hot Water' and 'Travel Mug' buttons."
    }

    ```

